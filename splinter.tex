\section{Splinter: Practical, Private Web Application Queries}
\label{chap:splinter}

This chapter presents Splinter, a practical system that protects user
queries to web applications.

\subsection{Motivation}
Many online services let users query large datasets:
some examples include restaurant sites, stock quotes,
medical information, and patents. In these services, 
any user can query the data, and the datasets themselves
do not contain sensitive user data. 
However, web services can infer a great deal of identifiable and sensitive
user information from these queries, such as her 
political affiliation, sexual orientation, income,
medical conditions, behavior, etc.~\cite{narayanan2010myths, narayanan2008robust}.
Web services can use this information maliciously and put users at risk to practices such as
discriminatory pricing~\cite{amazon-disc-pricing, price-disc2, hannak2014measuring}.
For example, online stores have charged users different prices based on location~\cite{price-disc}, and
travel sites have also increased prices for certain frequently searched flights~\cite{travel-pricing}.
Even when the services are honest, server compromise and subpoenas can leak the sensitive user
information collected by these services~\cite{ravichandran2009capturing, yelp-compromise, twitter-compromise}.

This paper presents Splinter, a system that protects users' queries to web applications
while achieving practical performance for many current web applications.
In Splinter, the user divides each query into shares and sends them to different
\emph{providers}, which are services hosting a copy of the dataset (Figure~\ref{fig:overview}).
As long as any one of the providers is honest and does not
collude with the others, the providers cannot discover sensitive
information in the query.
However, given responses from all the providers, the user
can compute the answer to her query.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{splinter-figs/overview.pdf}
	\caption[Overview of Splinter architecture.]{
		Splinter architecture. 
		The Splinter client splits each user query into shares and sends them to multiple
		providers. It then combines their results to obtain
		the final answer.
		The user's query remains private as long as any one provider is honest.
	}
	\label{fig:overview}
\end{figure}

Previous private query systems have generally not achieved practical performance
because they use expensive cryptographic primitives and protocols.
For example, systems based on Private Information Retrieval (PIR)~\cite{goldberg,chor1997private,pir-search} require many round trips and high bandwidth for complex queries, while systems based on garbled
circuits~\cite{wu2016,lan2016embark,ben2008fairplaymp} have a high computational cost.
These approaches are especially costly for mobile clients on high-latency networks.

Instead, Splinter is the first system to use and extend a recent cryptographic primitive called
Function Secret Sharing (FSS)~\cite{fss, gilboa2014distributed} for private queries.
FSS allows the client to split certain functions into shares that keep parameters of the
function hidden unless all the providers collude.
With judicious use of FSS, many queries can be answered at low CPU and bandwidth cost in only a single network round trip.

Splinter makes two contributions over previous work on FSS.
First, prior work has only demonstrated efficient FSS protocols for point and interval functions with additive aggregates such as SUMs~\cite{fss}.
We present protocols that support a more complex set of non-additive aggregates such as MAX/MIN and TOPK at low computational and communication cost.
Together, these protocols let Splinter support a subset of SQL that can capture many popular online applications.

Second, we develop an optimized implementation of FSS for modern hardware that leverages AES-NI~\cite{aes-ni} instructions and multicore CPUs.
For example, using the one-way compression functions that utilize modern AES instruction sets, our implementation is 2.5$\times$ faster per core than a na\"ive implementation of FSS.
Together, these optimizations let Splinter query datasets with millions of records at sub-second latency on a single server.

We evaluate Splinter by implementing 
three applications over it: a restaurant review site similar to Yelp, 
airline ticket search, and map routing.
For all of our applications, Splinter can execute queries in less than 1.6 seconds, at a cost of less than 0.005 cents in server resources on Amazon EC2.
%Splinter's low cost means that providers could profitably run a Splinter-based service
%similar to OpenStreetMap routing~\cite{osm}, an open-source maps service, while only charging users a few dollars per month.

%Finally, Splinter does have some limitations.
%First, FSS, like PIR, requires scanning the whole input dataset on
%every query, to prevent providers from figuring out which records have
%been accessed.
%Second, Splinter does not support some SQL features, such as private join conditions.
%Despite these limitations, we show that Splinter is practical on large
%real-world datasets, such as maps, and can support many of today's online applications.
%Because human-created datasets are unlikely to grow faster than
%hardware capabilities in the future, we believe Splinter's techniques will only
%become more practical over time.

%FSS improves over PIR 
%because it allows matching \textit{multiple} records efficiently in one scan
%while practical PIR schemes can only match a single record in one scan. 
%Therefore, to the best of our knowledge,
%Splinter is the first system to practically preserve the privacy 
%of queries on large public datasets.

%In summary, our contributions are:
%\begin{itemize}
%	\item{Splinter, a private query system that achieves significantly lower CPU and communication costs than previous systems.}
%	\item{New protocols that extend FSS to complex queries with non-additive aggregates, e.g., TOPK and MAX.}
%	\item{An optimized FSS implementation for modern CPUs.}
%	\item{An evaluation of Splinter on realistic applications.}
%\end{itemize}

\subsection{Splinter Architecture}
\label{sec:goals}

Splinter aims to protect sensitive information in users' queries
from providers. This section provides an overview of Splinter's architecture,
security goals, and threat model.

\subsubsection{Splinter Overview}
\label{sec:model}
There are two main principals in Splinter: the \emph{user} and the \emph{providers}.
Each provider hosts a copy of the data. Providers can 
retrieve this data from a public repository or mirror site.
Similarly, data owners, like Yelp and Google Maps, can license
data to be hosted on these providers.
%For example, OpenStreetMap~\cite{osm} publishes publicly available 
%map, point-of-interest, and traffic data. 
For a given user query, all the providers have to run it on the same
view of the data. Maintaining data consistency
from mirror sites is beyond the scope of this paper, but
standard techniques can be used~\cite{tewari2002wcdp,chi2008novel}.

As shown in Figure~\ref{fig:overview}, 
to issue a query in Splinter, a user
splits her query into \textit{shares}, using the Splinter client,
and submits each share to a different provider.
The user can select any providers of her choice that host the dataset.
The providers use their shares to execute the user's query 
over the cleartext data, using the Splinter provider library. 
As long as one provider is \textit{honest}
(does not collude with others), the user's sensitive information in the original query
remains private. When the user receives the responses from the providers,
she combines them to obtain the final answer to her original query. 
%In Section~\ref{sec:queries},
%we will describe in more detail how the user creates these function shares and how
%the provider uses them to execute the user's query.

\subsubsection{Security Goals}
\label{sec:query_model}
The goal of Splinter is to hide sensitive parameters in
a user's query.
Specifically, Splinter lets users run \emph{parametrized queries}, 
where both the parameters and query results are hidden from providers.
For example, consider the following query, which finds the 10 cheapest flights between a source and destination:
\begin{verbatim}
SELECT TOP 10 flightid FROM flights
WHERE source = ? AND dest = ? 
ORDER BY price
\end{verbatim}
Splinter hides the information represented by the questions marks, i.e.,
the source and destination in this example.
The column names being selected and filtered are not hidden.
Finally, Splinter also hides the query's results---otherwise,
these might be used to infer the source and destination. 
Splinter supports a subset of the SQL language, which we describe in Section~\ref{sec:querymodel}.

The easiest way to achieve this property would be for users to download the whole database
and run the queries locally. However, downloading so much data might not 
be feasible or efficient in terms of bandwidth and computation. Below,
we describe in more detail the specific use cases for Splinter.
%However, this requires substantial bandwidth and computation for the user.
%Moreover, many datasets change constantly, e.g., to include traffic information or new product reviews.
%It would be impractical for the user to continuously download these updates.
%Therefore, our performance objective is to minimize computation and communication costs.
%For a database of $n$ records, Splinter only requires $O(n \log n)$ computation at the
%providers and $O(\log n)$ communication (Section~\ref{sec:complex}).

\subsubsection{Use Cases}
Although Splinter can handle complex queries, it is not
meant as a private query system for all SQL-like databases.
Splinter's use cases are similar to those of PIR. More specifically,
the queries contain sensitive user information and need to be 
protected, but the database itself might not be sensitive. 

Splinter works well for applications with large databases
and regular updates because downloading the whole
database and receiving updates would be more expensive in terms
of bandwidth than querying the Splinter service. Services,
like flight prices, map navigation, stock prices, 
and domain name availability searches, are good fits for Splinter.
Even if it is more efficient for the user to download the whole
database and updates, Splinter can still be useful.
Many applications might enforce rate limiting or query restrictions
to prevent users from accessing the whole database
because the data might contain proprietary information. For example,
data owners like Google Maps and Yelp might allow limited querying
through an API but not give complete access to their dataset.

\subsubsection{Threat Model}
Splinter keeps the parameters in the user's query hidden
as long as at least one of the user-chosen providers does not collude with others. 
Splinter also assumes these providers are \textit{honest but curious}: a provider can observe the interactions between
itself and the client, but 
Splinter does not protect against providers returning incorrect results or maliciously modifying the dataset.

We assume that the user communicates with each provider through a secure channel (e.g., using SSL),
and that the user's Splinter client is uncompromised. 
%Protecting attacks on a user's machine are outside of the scope of Splinter.
Our cryptographic assumptions are standard.
We only assume the existence of one-way functions in our two-provider implementation.
In our implementation for multiple providers, the security of Paillier encryption~\cite{paillier} is also assumed.

\subsection{Function Secret Sharing}
\label{sec:queries}
In this section, we give an overview of Function Secret Sharing (FSS),
the main primitive used in Splinter, and show how to use it in simple queries.
Sections~\ref{sec:querymodel} and~\ref{sec:complex} then describe Splinter's full
query model and our new techniques for more complex queries.

\subsubsection{Overview of Function Secret Sharing}
\label{sec:fss_overview}
Function Secret Sharing~\cite{fss} lets a client divide
a function $f$ into \textit{function shares} 
$f_1,f_2,\dots,f_k$ so that multiple parties
can help evaluate $f$ without learning certain of its parameters.
These shares have the following properties:

\begin{itemize}
	\item{They are close in size to a description of $f$.}
	\item{They can be evaluated quickly (similar in time to $f$).}
	\item{They sum to the original function $f$.
		That is, for any input $x$, $\sum\limits_{i=1}^k f_i(x) = f(x)$. We 
		assume that all computations are done over $\mathbb{Z}_{2^m}$, 
		where $m$ is the number of bits in the output range. }
	\item{Given any $k-1$ shares $f_i$, an adversary cannot recover the parameters 
		of $f$.}
\end{itemize} 

Although it is possible to perform
FSS for arbitrary functions~\cite{dodis2016spooky}, 
practical FSS protocols only exist for \emph{point} and \emph{interval} functions.
These take the following forms:
\begin{itemize}
	\item Point functions $f_a$ are defined as $f_a(x)=1$ if $x=a$ or 0 otherwise.
	\item Interval functions are defined as $f_{a,b}(x)=1$ if $a \le x \le b$ or 0 otherwise.
\end{itemize}

In both cases, FSS keeps the parameters $a$ and $b$ private: an adversary can tell that
it was given a share of a point or interval function, but cannot find $a$ and $b$.
In Splinter, we use the FSS scheme of Boyle et al.~\cite{fss}.
Under this scheme, the shares $f_i$ for both functions require $O(\lambda n)$ bits to
describe and $O(\lambda n)$ bit operations to evaluate for a security parameter $\lambda$ (the size
of cryptographic keys), and $n$ is the number of bits in the input domain.
%This contrasts to $O(n)$ bits and operations to describe and evaluate the original
%functions.

\subsubsection{Using FSS for Database Queries}
\label{sec:fss_queries}

We can use the additive nature of FSS shares to run private queries over
an entire table in addition to a single data record.
We illustrate here with two examples.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{splinter-figs/fss.pdf}
	\caption{Overview of how FSS can be applied to database records
		on two providers to perform a COUNT query.}
	\label{fig:fss_overview}
\end{figure}


\paragraph{Example: COUNT query.}
Suppose that the user wants to run the following query on
a table served by Splinter:
\begin{verbatim}
SELECT COUNT(*) FROM items WHERE ItemId = ?
\end{verbatim}

Here, `\texttt{?}' denotes a parameter that the user would like to keep
private; for example, suppose the user is searching for \texttt{ItemId = 5},
but does not want to reveal this value.

To run this query, the Splinter client defines a point function $f(x)=1$ if $x=5$
or 0 otherwise.
It then divides this function into function shares $f_1,\dots,f_n$ and
distributes them to the providers, as shown in Figure~\ref{fig:fss_overview}.
For simplicity, suppose that there are two providers, who receive shares
$f_1$ and $f_2$.
Because these shares are additive, we know that $f_1(x)+f_2(x)=f(x)$
for every input $x$.
Thus, each provider $p$ can compute $f_p(\mathrm{ItemId})$ for every ItemId in the
database table, and send back $r_p = \sum_{i=1}^n f_p(\mathrm{ItemId}_i)$
to the client.
The client then computes $r_1 + r_2$, which is equal
to $\sum_{i=1}^n f(\mathrm{ItemId}_i)$, that is, the count of all matching
records in the table.

\begin{figure}
	\centering
		\begin{tabular}{cccc}
			\toprule
			\bf ItemId & \bf Price & $f_1$(ItemId) & $f_2$(ItemId) \\
			\midrule
			5 & 8 & 10 & -9 \\
			1 & 8 & 3 & -3 \\
			5 & 9 & 10 & -9 \\
			\bottomrule
		\end{tabular}
	\caption[Function Secret Sharing example outputs.]{Simple example table with outputs for the FSS function shares $f_1$, $f_2$ applied to the ItemId column. 
		The function
		is a point function that returns 1 if the input is 5, and 0 otherwise.
		All outputs are integers modulo $2^m$ for some $m$.
	}
	\label{fig:fssExample2}
\end{figure}

To make this more concrete, Figure~\ref{fig:fssExample2} shows an example
table and some sample outputs of the function shares, $f_1$ and $f_2$,
applied to the ItemId column. 
There are a few important observations. First, to each provider,
the outputs of their function share seem random. Consequently, the provider does not learn
the original function $f$ and the parameter ``5''. Second, 
because $f$ evaluates 
to 1 on inputs of 5, $f_1(\mathrm{ItemId}) + f_2(\mathrm{ItemId}) = 1$ for rows 1 and 3. 
Similarly, $f_1(\mathrm{ItemId}) + f_2(\mathrm{ItemId})=0$ for row 2.
Therefore, when summed across the providers, each row
contributes 1 (if it matches) or 0 (if it does not match) to the 
final result. 
Finally, each provider aggregates the outputs of their shares by summing them. 
In the example, one provider returns 23 to the client, and
the other returns -21.
The sum of these is the correct query output, 2.

This additivity of FSS enables Splinter
to have \textit{low communication costs} for aggregate queries, by aggregating
data locally on each provider.
%Adding values locally in the provider 
%does not leak any information because each value appears random to the provider.

\paragraph{Example: SUM query.}
Suppose that instead of a COUNT, we wanted to run the following SUM query:
\begin{verbatim}
SELECT SUM(Price) FROM items WHERE ItemId=?
\end{verbatim}

This query can be executed privately with a small extension to the COUNT scheme.
As in COUNT, we define a point function $f$ for our secret predicate, e.g.,
$f(x)=1$ if $x=5$ and 0 otherwise.
We divide this function into shares $f_1$ and $f_2$.
However, instead of computing
$r_p = \sum_{i=1}^n f_p(\mathrm{ItemId}_i)$,
each provider $p$ computes
$$r_p = \sum_{i=1}^n f_p(\mathrm{ItemId}_i) \cdot \mathrm{Price}_i$$

As before, $r_1 + r_2$ is the correct answer of the query,
that is, $\sum_{i=1}^n f(\mathrm{ItemId}_i) \cdot \mathrm{Price}_i$.
We add in each row's price, $\mathrm{Price}_i$, 0 times if the ItemId is equal to 5,
and 1 time if it does not equal 5.

\subsection{Splinter Query Model}
\label{sec:querymodel}

Beyond the simple SUM and COUNT queries in the previous section, 
we have developed
protocols to execute a large class of queries using FSS, including
non-additive aggregates such as MAX and MIN, and queries that return multiple
individual records instead of an aggregate.
For all these queries, our protocols are efficient in both
computation and communication.
On a database of $n$ records, all queries can be executed in $O(n \log n)$ time
and $O(\log n)$ communication rounds, and most only require 1 or 2 communication rounds
(Figure~\ref{fig:complexity} on page~\pageref{fig:complexity}).

\begin{figure}
	%\small
	\centering
	\fbox{
		\begin{minipage}{0.7\textwidth}
			\begin{description}
				\item[\normalfont Query format:] \hspace{1mm} \\
				SELECT \emph{aggregate}$_1$, \emph{aggregate}$_2$, $\dots$ \\
				FROM \emph{table}\\
				WHERE \emph{condition}\\
				$[$GROUP BY \emph{expr}$_1$, \emph{expr}$_2$, $\dots]$
				
				\item[\normalfont \it aggregate:] \mbox{}\\[-1.5\baselineskip]
				\begin{itemize}
					\item COUNT | SUM | AVG | STDEV (\emph{expr})
					\item MAX | MIN (\emph{expr})
					\item TOPK (\emph{expr}, $k$, \emph{sort\_expr})
					\item HISTOGRAM (\emph{expr}, \emph{bins})
				\end{itemize}
				
				\item[\normalfont \it condition:] \mbox{}\\[-1.5\baselineskip]
				\begin{itemize}
					\item \emph{expr} = \emph{secret}
					\item \emph{secret}$_1$ $\le$ \emph{expr} $\le$ \emph{secret}$_2$
					\item AND of `=' conditions and up to one interval
					\item OR of multiple disjoint conditions\\
					(e.g., \texttt{country="UK" OR country="USA"})
				\end{itemize}
				
				\item[\normalfont \it expr:] any public function of the fields in a table
				row\\(e.g., \texttt{ItemId + 1} or \texttt{Price * Tax})
				
			\end{description}
		\end{minipage}
	}
	
	\caption[Splinter query format.]{Splinter query format.
		The TOPK aggregate returns the top $k$ values of \emph{expr} for matching rows
		in the query, sorting them by \emph{sort\_expr}.
		In conditions, the parameters labeled \emph{secret} are hidden from the
		providers.
	}
	\label{fig:suppq}
\end{figure}

Figure~\ref{fig:suppq} describes Splinter's supported queries using SQL syntax.
Most operators are self-explanatory.
The only exception is TOPK, which is used to return up to $k$ individual records
matching a predicate, sorting them by some expression \emph{sort\_expr}.
This operator can be used to implement \texttt{SELECT...LIMIT} queries, but
we show it as a single ``aggregate'' to simplify our exposition.
To keep the number of matching records hidden from providers, the
protocol always pads its result to exactly $k$ records.

%Note also that all queries return a bounded amount of data---there is no way to
%select arbitrarily many rows.
%This is expected given our security model: if we allowed variable-length
%results, the providers might learn something about the query from the number
%of rows sent back.

Although Splinter does not support all of SQL, we found it expressive enough to
support many real-world query services over public data.
We examined various websites, including Yelp, Hotels.com, and Kayak, and found
we can support most of their search features as shown in Section~\ref{sec:case_studies}.

Finally, Splinter only ``natively'' supports fixed-width integer data
types. However, such integers can also be used to encode strings and
fixed-precision floating point numbers (e.g., SQL DECIMALs).
We use them to represent other types of data in our sample applications.

\subsection{Executing Splinter Queries}
\label{sec:complex}

Given a query in Splinter's query format (Figure~\ref{fig:suppq}), the system executes
it using the following steps:

\begin{enumerate}
	\item The Splinter client builds function shares for the condition in the query,
	as we shall describe in Section~\ref{sec:complex-conditions}.
	\item The client sends the query with all the secret parameters removed to
	each provider, along with that provider's share of the condition function.
	\item If the query has a GROUP BY, each provider divides its data into
	groups using the grouping expressions; otherwise, it treats the whole table
	as one group.
	\item \label{agg-step}
	For each group and each aggregate in the query, the provider runs an
	evaluation protocol that depends on the aggregate function and on
	properties of the condition. We describe these protocols in
	Section~\ref{sec:complex-aggregates}.
	Some of the protocols require
	further communication with the client, in which case the provider
	batches its communication for all grouping keys together.
\end{enumerate}

The main challenge in developing Splinter is designing efficient execution protocols
for Splinter's complex conditions and aggregates (Step~\ref{agg-step}).
Our contribution is multiple protocols that can execute non-additive
aggregates with low computation and communication costs.

One key insight that pervades our design is that \emph{the best strategy to
	compute each aggregate depends on properties of the condition function}.
For example, if we know that the condition can only match one value of
the expression it takes as input, we can simply compute the aggregate's result
for \emph{all} distinct values of the expression in the data, and then use a point
function to return just one of these results to the client.
On the other hand, if the condition can match multiple values, we need a
different strategy that can combine results across the matching values.
To reason about these properties, we define three \emph{condition classes}
that we then use in aggregate evaluation.

\subsubsection{Condition Types and Classes}
\label{sec:complex-conditions}

For any condition $c$, the Splinter client defines a function $f_c$
that evaluates to 1 on rows where $c$ is true and 0 otherwise, and divides
$f_c$ into shares for each provider.
Given a condition $c$, let $E_c = (e_1, \dots, e_t)$ be the list of
expressions referenced in $c$ (the \emph{expr} parameters in its clauses).
Because the best strategy for evaluating aggregates depends on $c$, we
divide conditions into three classes:
\begin{itemize}
	\item \emph{Single-value conditions.} These are conditions that can only be
	true on one combination of the values of $(e_1, \dots, e_t)$.
	For example, conditions consisting of an AND of `='
	clauses are single-value.
	\item \emph{Interval conditions.} These are conditions where the input
	expressions $e_1,\dots,e_t$ can be ordered such that $c$ is true on an
	interval of the range of values $e_1||e_2||\dots||e_t$ (where $||$ denotes
	string concatenation).
	\item \emph{Disjoint conditions,} i.e., all other conditions.
\end{itemize}

The condition types described in our query model (Figure~\ref{fig:suppq})
can all be converted into sharable functions, and categorized into these classes,
as follows:

\paragraph{Equality-only conditions.}
Conditions of the form
$\mathit{e}_1=\mathit{secret}_1$ AND $\dots$ AND
$\mathit{e}_t=\mathit{secret}_t$
can be executed as a single point function on the binary string
$\mathit{e}_1||\dots||\mathit{e}_t$.
This is simply a point function that can be shared using existing
FSS schemes~\cite{fss}.
These conditions are also single-value.

\paragraph{Interval and equality.}
Conditions of the form
$\mathit{e}_1=\mathit{secret}_1$ AND $\dots$ AND
$\mathit{e}_{t-1}=\mathit{secret}_{t-1}$ AND
$\mathit{secret}_{t} \le \mathit{e}_{t} \le \mathit{secret}_{t+1}$
can be executed as a single interval function on the binary string
$\mathit{e}_1||\dots||\mathit{e}_t$.
This is again supported by existing FSS schemes~\cite{fss}, and is
an interval condition.

\paragraph{Disjoint OR.}
Suppose that $c_1,\dots,c_t$ are \emph{disjoint} conditions that can be
represented using functions $f_{c_1}, \dots, f_{c_t}$.
Then $c = c_1$ OR$\dots$OR $c_t$ is captured by
$f_c = f_{c_1} + \dots + f_{c_t}$.
We share this function across providers by simply giving them shares of
the underlying functions $f_{c_i}$.
In the general case, however, $c$ is a disjoint condition where we
cannot say much about which inputs give 0 or 1.

%\footnote{
%Note also that adding the $f_i$s does not work for overlapping OR
%clauses. However, disjoint clauses are still
%useful for many real-world queries, e.g., searching for restaurants that are
%either Chinese or Indian.
%\}

\subsubsection{Aggregate Evaluation}
\label{sec:complex-aggregates}

%Our aggregate evaluation protocols depend on both the aggregate function and the
%condition class in the query.

\subsubsubsection{Sum-Based Aggregates}
To evaluate SUM, COUNT, AVG, STDEV and HISTOGRAM, Splinter
sums one or more values for each row regardless of the condition
function class.
For SUM and COUNT, each provider sums the expression being aggregated or a 1 for each
row and multiplies it by $f_i(\mathrm{row})$, its share of the condition
function, as in Section~\ref{sec:fss_queries}.
Computing AVG($x$) for an expression $x$, requires finding SUM($x$) and COUNT($x$),
while computing STDEV($x$) requires finding these values and SUM($x^2$).
Finally, computing a HISTOGRAM into bin boundaries provided by the user simply
requires tracking one count per bin, and adding each row's result to the
count for its bin. Note that the binning expression is not
private---only information about which rows pass the query's condition function.

\subsubsubsection{MAX and MIN}
\label{sec:min-max}
Suppose we are given a query to find MAX($e_0$) WHERE $c(e_1,\dots,e_t)$,
for expressions $e_0,\dots,e_t$.
The best evaluation strategy depends on the class of the condition $c$.

\paragraph{Single-value conditions.} If $c$ is only true for one combination
of the values $e_1,\dots,e_t$, each provider starts by evaluating the query
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
SELECT MAX($e_0$) FROM data GROUP BY $e_1,\dots,e_t$
\end{Verbatim}
This query gives an \emph{intermediate table} with the tuples $(e_1,\dots,e_t)$
as keys and $\mathrm{MAX}(e_0)$ as values.
Next, each provider computes $\sum \mathrm{MAX}(e_0) \cdot f_i(e_1,\dots,e_t)$
across the rows of the intermediate table, where $f_i$ is its share of the
condition function.
This sum will add a 0 for each non-matching row and $\mathrm{MAX}(e_0)$ for the
matching row, thus returning the right value.
Note that if the original table had $n$ rows, the intermediate table can be built
in $O(n)$ time and space using a hash table.

\paragraph{Interval conditions.}
Suppose that $c$ is true if and only if $e_1||\dots||e_t$ is in an interval $[a,b]$,
where $a$ and $b$ are secret parameters.
As in the single-value case, the providers can build a data structure that helps
them evaluate the query without knowing $a$ and $b$.
%The protocol runs as follows:

In this case, each provider builds an array $A$ of entries $(k,v)$, where the keys are all
values of $e_1||\dots||e_t$ in lexicographic order, and the values are $\mathrm{MAX}(e_0)$
for each key.
It then computes $\mathrm{MAX}(A[i..j])$ for all \emph{power-of-2 aligned} intervals
of the array $A$ (Figure~\ref{fig:complex-max-tree}).
This data structure is similar to a Fenwick tree~\cite{fenwick-trees}.

%For example, if $A$ has 6 elements, the provider will compute MAXes on
%$A[0..1]$, $A[2..3]$, $A[4..5]$ and $A[0..3]$.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{splinter-figs/complex_max_tree.pdf}
	\caption[Data structure for querying MAX on intervals.]{Data structure for querying MAX on intervals.
		We find the MAX on each power-of-2 aligned interval in the array,
		of which there are $O(n)$ total.
		Then, any interval query requires retrieving $O(\log n)$ of these
		values. For example, to find $MAX(A[3..6])$, we need two
		size-1 intervals and one size-2.
	}
	\label{fig:complex-max-tree}
\end{figure}

Query evaluation then proceeds in two rounds.
First, Splinter counts how many keys in $A$ are less than $a$ and how many are less than
$b$: the client sends the providers shares of the interval functions
$k \in [0,a-1]$ and $k \in [0,b-1]$, and the providers apply these to all keys $k$
and return their results.
This lets the client find indices $i$ and $j$ in $A$
such that all the keys $k \in [a,b]$ are in $A[i..j]$.

Second, the client sends each provider shares of new point functions that select
up to two intervals of size 1, up to two intervals of size 2, etc out of the
power-of-2 sized intervals that the providers computed MAXes on, so as to cover
exactly $A[i..j]$.
Note that any integer interval can be covered using at most 2 intervals of each power of 2.
The providers evaluate these functions to return the MAXes for the selected intervals,
and the client combines these $O(\log n)$ MAXes to find the overall MAX on $A[i..j]$.\footnote{
	To hide which sizes of intervals were actually required, the client should always request 2 intervals
	of each size and ignore unneeded ones.
}

For a table of size $n$, this protocol requires $O(n \log n)$ time at each provider
(to sort the data to build $A$, and then to answer $O(\log n)$ point function queries).
It also only requires two communication rounds, and $O(\log n)$ communication bandwidth.
The same protocol can be used for other associative aggregates, such as products.

\paragraph{Disjoint conditions.}
If we must find MAX($e_0$) WHERE $c(e_1,\dots,e_t)$ but know nothing about $c$,
Splinter builds an array $A$ of all rows in the dataset sorted by $e_0$.
Finding MAX($e_0$) WHERE $c$ is then equivalent to finding the largest index $i$
in $A$ such that $c(A[i])$ is true.
To do this, Splinter uses binary search.
The client repeatedly sends private queries of the form
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
SELECT COUNT(*) FROM $A$
WHERE $c(e_1,\dots,e_t)$ AND $\mathit{index}\in[\mathit{secret}_1,\mathit{secret}_2],$
\end{Verbatim}
where \textit{index} represents the index of each row in $A$ and the interval for it is kept
private. 
%(We shall discuss how to do this.)
By searching for secret intervals in decreasing power-of-2 sizes, the client can find the
largest index $i$ such that $c(A[i])$ is true.
For example, if we had an array $A$ of size 8 with largest matching element at $i=5$,
the client would probe $A[0..3]$, $A[4..7]$, $A[4..5]$, $A[6..7]$ and finally $A[4]$ to
find that 5 is the largest matching index.

Normally, ANDing the new condition $\mathit{index}\in[\mathit{secret}_1,\mathit{secret}_2]$
with $c$ would cause problems, because the resulting conditions might no longer be in
Splinter's supported condition format (ANDs with at most one interval and ORs of
disjoint clauses).
Fortunately, because the intervals in our condition are always power-of-2 aligned,
it can also be written as an equality on the first $k$ bits of \emph{index}.
For example, supposing that \emph{index} is a 3-bit value, the condition
$\mathit{index} \in [4,5]$ can be written as $\mathit{index}_{0,1} =$ ``10'', where
$\mathit{index}_{0,1}$ is the first two bits of \emph{index}.
This lets us AND the condition into all clauses of $c$.

Once the client has found the largest matching index $i$, it runs one
more query with a point function to select the row with $index = i$.
The whole protocol requires $O(\log n)$ communication rounds
and $O(n \log n)$ computation and works well if $c$ has many conditions. 

However, if $c$ has a small number of OR clauses, an optimization is to run
one query for each clause in parallel. The user then resolves
the responses locally to find the answer to the original query.
Although doing this optimization requires more bandwidth because the returned
result size is larger, it avoids the $O(\log n)$ communication rounds and 
the $O(n\log n)$ computation.

\subsubsubsection{TOPK}
\label{sec:topk}
Our protocols for evaluating TOPK are similar to those for MAX and MIN.
Suppose we are given a query to find TOPK($e$, $k$, $e_\mathrm{sort}$) WHERE $c(e_1,\dots,e_t)$.
The evaluation strategy depends on the class of the condition $c$.

\paragraph{Single-value conditions.} If $c$ is only true for one combination
of $e_1,\dots,e_t$, each provider starts by evaluating
\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`$=3\catcode`_=8}]
SELECT TOPK($e,$ $k,$ $e_\mathrm{sort}$) FROM data
GROUP BY $e_1,\dots,e_t$
\end{Verbatim}
This gives an intermediate table with the tuples $(e_1,\dots,e_t)$
as keys and TOPK$(\cdot)$ for each group as values, from which we can select
the single row matching $c$ as in MAX.

\paragraph{Interval conditions.}
Here, the providers build the same auxiliary array $A$ as
in MAX, storing the TOPK for each key instead.
They then compute the TOPKs for power-of-2 aligned intervals in this array.
The client finds the interval $A[i..j]$ it needs to query, extracts the top $k$ values
for power-of-2 intervals covering it, and finds the overall top $k$.
As in MAX, this protocol requires 2 rounds and $O(\log n)$ communication bandwidth.

\paragraph{Disjoint conditions.}
Finding TOPK for disjoint conditions is different from MAX because we need to return
multiple records instead of just the largest record in the table that matches $c$.
This protocol proceeds as follows:
\begin{enumerate}
	\item The providers sort the whole table by $e_\mathrm{sort}$ to create an auxiliary array $A$.
	\item The client uses binary search to find indices $i$ and $j$ in $A$ such that the top $k$
	items matching $c$ are in $A[i..j]$. This is done the same way as in MAX, but searching for
	the largest indices where the count of later items matching $c$ is 0 and $k$.
	\item The client uses a sampling technique (Section~\ref{sec:sampling}) to extract
	the $k$ records from $A[i..j]$ that match $c$. Intuitively, although we do not know which rows
	these are, we build a result table of $>k$ values initialized to 0, and add the FSS share
	for each row of the data to one row in the result
	table, chosen by a hash. This scheme extracts all matching
	records with high probability.
\end{enumerate}
This protocol needs $O(\log n)$ communication rounds
and $O(n \log n)$ computation if there are many clauses, but like the protocol for MAX,
if the number of clauses in $c$ is small,
the user can issue parallel queries for each clause to reduce the communication rounds
and computation.

\subsubsection{Extracting Disjoint Records with FSS}
\label{sec:sampling}

Here, we describe our sampling-based technique for returning multiple records using FSS, used in TOPK queries with disjoint conditions (Section~\ref{sec:topk}).
Given a table $T$ of records and a condition $c$ that matches up to $k$ records, we wish to return those records to the client with high probability without revealing $c$.

To solve this problem, the providers each create a result table $R$ of size $l > k$, containing (value, count) columns all initialized to 0.
They then iterate through the records and choose a result row to update for each record based on a hash function $h$ of its index $i$.
For each record $r$, each provider adds $1 \cdot f_c(r)$ to $R[h(i)]$.count
and $r \cdot f_c(r)$ to $R[h(i)]$.value, where $f_c$ is its share of the condition $c$.
The client then adds up the $R$ tables from all the providers to build up a single table,
which contains a value and count for all indices that a record matching $c$ hashed into.

Given this information, the client can tell how many records hashed into each
index: entries with count=1 have only one record, which can be read from the entry's value.
Unfortunately, entries with higher counts hold multiple records that were added together in
the value field.
To recover these entries, the client can run the same process multiple times in parallel with
different hash functions $h$.

In general, for any given value of $r$ and $k$, the probability of a given record colliding with another under each hash function is a constant (e.g., it is less than $1/3$ for $r=3k$).
Repeating this process with more hash functions causes the probability to fall exponentially.
Thus, for any $k$, we can return all the distinct results with high probability using only $O(\log k)$
hash functions and hence only $O(\log k)$ extra communication bandwidth.

\subsubsection{Complexity}

\begin{figure}
	\centering
		\begin{tabular}{ccccc}
			\toprule
			\bf Aggregate & \bf Condition & \bf Time & \bf Rounds & \bf Bandwidth \\
			\midrule
			Sum-based & any & $O(n)$ & $1$ & $O(1)$ \\
			\midrule
			MAX/MIN & 1-value & $O(n)$ & $1$ & $O(1)$ \\
			MAX/MIN & interval & $O(n \log n)$ & $2$ & $O(\log n)$ \\
			MAX/MIN & disjoint & $O(n \log n)$ & $O(\log n)$ & $O(\log n)$ \\
			\midrule
			TOPK & 1-value & $O(n)$ & $1$ & $O(1)$ \\
			TOPK & interval & $O(n \log n)$ & $2$ & $O(\log n)$ \\
			TOPK & disjoint & $O(n \log n)$ & $O(\log n)$ & $O(\log n)$ \\
			\bottomrule
		\end{tabular}
	\caption[Complexity of Splinter's query evaluation protocols.]{Complexity of Splinter's query evaluation protocols for a database of size $n$.
		For bandwidth, we report the multiplier over the query's normal result size.
	}
	\label{fig:complexity}
\end{figure}

Figure~\ref{fig:complexity} summarizes the complexity of Splinter's query evaluation protocols
based on the aggregates and condition classes used.
We note that in all cases, the computation time is $O(n \log n)$ and the communication costs
are much smaller than the size of the database.
This makes Splinter practical even for databases with millions of records, which covers
many common web application datasets, as shown in Section~\ref{sec:evaluation}.
Finally, the main operations used to evaluate Splinter queries at providers, namely sorting
and sums, are highly parallelizable, letting Splinter take advantage of parallel hardware.

\subsection{Optimized FSS Implementation}
\label{sec:fastfss}

Apart from introducing new protocols to evaluate complex queries using FSS,
Splinter includes an FSS implementation optimized for modern hardware.
In this section, we describe our implementation and also discuss
how to select the best multi-party FSS scheme for a given query.

The two-party FSS protocol~\cite{fss} is efficient
because of its use of one-way functions.
A common class of one-way functions is 
pseudorandom generators (PRGs)~\cite{levin1987one}, 
and in practice, 
AES is the most commonly used PRG because of hardware accelerations,
i.e. the AES-NI~\cite{aes-ni} instruction. 
Generally, using AES as a PRG is straightforward (use
AES in counter mode). However, the use of PRGs 
in FSS is not only atypical, but it also represents a large portion
of the computation cost in the protocol. The FSS protocol 
requires many instantiations 
of a PRG with different initial seed 
values, especially in the two-party 
protocol~\cite{fss}. Initializing multiple PRGs with
different seed values is very 
computationally expensive because AES cipher
initialization is \textit{much slower} than performing
an AES evaluation on an input. Therefore, 
the challenge in Splinter is to find an efficient PRG
for FSS.

Our solution is to use \textit{one-way compression functions}.
One way compression functions are commonly used as a primitive
in hash functions, like SHA, and are built using a block
cipher like AES. In particular, Splinter uses the Matyas-Meyer-Oseas one-way 
compression function~\cite{matyas1985} because
this function utilizes a \textit{fixed key} cipher. As a result, 
the Splinter protocol initializes the cipher only once per query.

More precisely, the Matyas-Meyer-Oseas one-way compression
function is defined as:
\begin{equation*}
F(x) = E_k(x) \oplus x
\end{equation*}
where $x$ is the input, i.e. PRG seed value, and $E$ is a block cipher with a fixed key $k$.

The output of a one-way compression function 
is a fixed number of bits, but we can
use multiple one-way compression functions with
different keys and concatenate the outputs to obtain more bits.
Security is preserved because a function that
is a concatenation of one-way functions is still a one-way function.

With this one-way compression function, Splinter initializes the cipher, $E_k$,
at the beginning of the query and reuses it
for the rest of the query, avoiding
expensive AES initialization operations in the FSS protocol. 
For each record, the Splinter protocol 
needs to perform only $n$ XORs and $n$ AES
evaluations using the AES-NI instruction, where
$n$ is the input domain size of the record. 
In Section~\ref{sec:micro}, we show that Splinter's use of one-way compression functions
results in a 
$2.5\times$ speedup over using AES directly as a PRG.

\subsection{Implementation}
\label{sec:implementation}

We implemented Splinter in C++, using OpenSSL 1.0.2e~\cite{openssl} 
and the AES-NI hardware instructions
for AES encryption. We used 
GMP~\cite{gmp} for large integers and OpenMP~\cite{openmp} for multithreading.
Our optimized FSS library is about 2000 lines of code, and the applications on top of it are about 2000 lines of code. There is around 
1500 lines of test code to issue the queries. For comparison, 
we also implement the multi-party
FSS scheme in~\cite{corrigan-gibbs:riposte} using 2048 bit Paillier encryption ~\cite{paillier}.
Our FSS library implementation can be found at \url{https://github.com/frankw2/libfss}.



\subsection{Discussion and Limitations}
\label{spl-sec:discussion}

\paragraph{Economic feasibility:}
\label{sec:disc-economics}

Although it is hard to predict real-world deployment, we believe that Splinter's low cost makes it economically feasible for several types of applications.
Here are some possible methods for monetization.
For example, despite many current data owners, such as Yelp and Google Maps, generating revenue primarily by showing ads and mining user data,
they can license their data to Splinter providers and have these providers manage a Splinter deployment. The providers
can then charge a subscription fee, e.g. \$1 a month, for usage of the server.
Similarly, these providers can collectively issue a utility token that
can be used to pay for the queries.
Splinter's trust model, where only one provider needs to be honest, also makes 
it easy for new providers to join the market, increasing users' privacy.

This businesses seems reasonable as studies have shown that 
many consumers are willing to pay for services that protect their privacy~\cite{atlantic,atlantic2}. 
In fact, users might not use certain services because of privacy concerns~\cite{ravichandran2009capturing,riley2008tolls}.
Similarly, more users are using sites like DuckDuckGo and technologies like Tor 
because they are unwilling to have sites track their
query behavior, which shows a growing consumer market for privacy-preserving technologies. 
However, whether such a business model would work or be feasible 
in practice is beyond the scope of this dissertation.
%Well-known sites like OkCupid, Pandora, Youtube, and Slashdot allow users to 
%pay a monthly fee to remove ads that collect their information, showing there is
%already a demographic willing to pay for privacy. Moreover,

%As shown in Section~\ref{sec:pricing}, the cost of running queries on Splinter is low, with our most expensive query, map routing, costing less than 0.005 cents in AWS resources.
%At this cost, providers could offer Splinter-based map routing for a subscription fee of \$1 per month.
%Moreover, the availability of techniques like Splinter might make it easier to introduce 
%regulation about privacy in certain settings, similar to current privacy regulations in HIPAA~\cite{hipaa}
%and GDPR~\cite{gdpr}.

%Nonetheless, there are already successful open databases containing most of the data in these services, such as OpenStreetMap~\cite{osm}, and basic data on locations does not change rapidly once collected.
%There are already multiple Android and iOS applications that download subsets of OpenStreetMap data for  offline routing and update it periodically~\cite{osm-offline}.

%\subsection{Extensions to Splinter}
%\label{sec:disc-extensions}
%
%To support more workloads, Splinter's query model and evaluation algorithms can be extended in several ways.

%Finally, Splinter does have some limitations.
%First, FSS, like PIR, requires scanning the whole input dataset on
%every query, to prevent providers from figuring out which records have
%been accessed.
%Second, Splinter does not support some SQL features, such as private join conditions.
%Despite these limitations, we show that Splinter is practical on large
%real-world datasets, such as maps, and can support many of today's online applications.
%Because human-created datasets are unlikely to grow faster than
%hardware capabilities in the future, we believe Splinter's techniques will only
%become more practical over time.

\paragraph{Unsupported queries:}
As shown in Section~\ref{sec:querymodel}, Splinter supports only a subset of SQL.
Splinter does not support partial text matching or image matching, which are common in types of applications
that might use Splinter. Moreover, Splinter cannot support private joins, i.e. Splinter can only support joining with 
another table if the join condition is public, which encompasses a large majority of join operations. 
Despite these limitations, our study in Section~\ref{sec:case_studies} 
shows Splinter can support many application search interfaces.

\paragraph{Number of providers:} 
One limitation of Splinter is that 
a Splinter-based service has 
to be deployed on at least two providers. 
However, previous PIR systems described in Section~\ref{sec:related}
also require at least two providers. 

%Unlike those systems, 
%Splinter requires only \textit{one} honest provider whereas those systems
%require \textit{all} providers be honest. Moreover,
%current multi-party FSS schemes do not scale well past 
%three providers, but we believe that further research will improve its efficiency.

%\paragraph{Joins:}
%As shown in Section~\ref{sec:querymodel}, Splinter can support only joining with another table 
%if the join condition is public,
%i.e. the join condition cannot contain sensitive or private information. To do this, 
%providers can run the join before filtering the data using the private condition.
%We leave the development of private join conditions to future work.

\paragraph{Full table scans:}
FSS, like PIR, requires scanning the whole input dataset on every Splinter query,
to prevent providers from figuring out which records have been accessed. 
Despite this limitation, we have shown that Splinter is practical
on large real-world datasets, such as maps.

Splinter needs to scan the whole table only for conditions 
that contain sensitive parameters.
For example, consider the query:
\begin{verbatim}
SELECT flight from table WHERE src=SFO 
AND dst=LGA AND delay < 20
\end{verbatim}
If the user does not consider the delay of 20 in this query to be
private, Splinter could send it in the clear.
The providers can then create an intermediate
table with only flights where the delay $<$ 20 and apply the private
conditions only to records in this table.
In a similar manner, users querying geographic data may be willing to
reveal their location at the country or state level but would like to
keep their location inside the state or country private.

\paragraph{Maintaining consistent data views:}
Splinter requires that each provider executes a given user
query on the same copy of the data. 
Much research in distributed systems has focused on ensuring
databases consistency across multiple providers~\cite{spanner, ongaro:raft, tu:silo}.
Using the appropriate consistency techniques is dependent
on the application and an active area of research.
Applying those techniques in Splinter is beyond the scope
of this paper.


\subsection{Related Work}
\label{spl-sec:related}
Splinter is related to work in Private Information Retrieval (PIR),
garbled circuit systems, encrypted data systems, 
and Oblivious RAM (ORAM) systems. Splinter achieves higher performance than these systems 
through its mapping of database queries to the Function Secret Sharing (FSS) primitive.

\paragraph{PIR systems:}
Splinter is most closely related to systems that use Private
Information Retrieval (PIR)~\cite{chor1998private} to query a database privately.
In PIR, a user queries for the $i^\mathrm{th}$ record in the database, and the database
does not learn the queried index $i$ or the result.
Much work has been done on improving 
PIR protocols~\cite{ostrovsky2007survey, olumofin2011revisiting}. 
Work has also been done to extend PIR to return multiple records~\cite{groth2010multi},
but it is computationally expensive.
Our work is most closely related to the system in~\cite{goldberg}, which implements
a parametrized SQL-like query model similar to Splinter using PIR, but requires
more computation than Splinter.

Popcorn~\cite{popcorn} is a media delivery service that 
uses PIR to hide user consumption habits from the provider
and content distributor. However, Popcorn is optimized for
streaming media databases, like Netflix, which have a small number (about 8000)
of large records. 

The systems above have a weaker
security model: \textit{all} the providers need to be honest.
Splinter only requires \textit{one} honest provider, and
it is more practical because it 
extends Function Secret Sharing (FSS)~\cite{fss,gilboa2014distributed}, which lets
it execute complex operations such as sums in one round trip
instead of only extracting one data record at a time.

\paragraph{Garbled circuits:}
Systems such as Embark~\cite{lan2016embark}, BlindBox~\cite{blindbox}, 
and private shortest path computation systems~\cite{wu2016}
use garbled circuits~\cite{Yao, goldwasser1997multi} to perform private computation
on a single untrusted server.
Even with improvements in practicality~\cite{bellare2013efficient}, these
techniques still have high computation and bandwidth costs for queries on
large datasets because a new garbled circuit has to be generated for each query.
(Reusable garbled circuits~\cite{goldwasser:sfe} are not yet practical.)
%For example, the recent map routing system by Wu et al.~\cite{wu2016} uses garbled circuits and 
%has $100\times$ higher response time and $10\times$ higher bandwidth cost
%than Splinter.

\paragraph{Encrypted data systems:}
Systems that compute on encrypted data, like 
CryptDB~\cite{popa:cryptdb}, Mylar~\cite{popa:mylar}, SPORC~\cite{feldman:sporc},
Depot~\cite{mahajan:depot}, and SUNDR~\cite{li:sundr}, all try to protect
private data against a server compromise, which is a different
problem than what Splinter tries to solve. CryptDB is most similar to Splinter 
because it allows for SQL-like queries over encrypted data. However, all
these systems protect against a single, potentially compromised server 
where the user is storing data privately, but they do not hide data access patterns. 
In contrast, Splinter hides data access patterns and a user's query parameters 
but is only designed to operate on a cleartext 
datasets that is hosted at multiple providers.


\paragraph{ORAM systems:}
Splinter is also related to systems that use Oblivious RAM~\cite{stefanov:path-oram, lorch2013shroud}. 
ORAM allows a user to read and write data on an untrusted server without
revealing her data access patterns to the server. However, ORAM cannot be easily applied
into the Splinter setting. One main requirement of ORAM is that the user
can only read data that she has written. 
In Splinter, the provider hosts a cleartext dataset, not created by any specific user, 
and many users need to access the same dataset.

\subsection{Conclusion}
\label{spl-sec:conclusion}
Splinter is a new private query system that protects sensitive parameters
in SQL-like queries while scaling to realistic applications. Splinter uses and extends a recent
cryptography primitive, Function Secret Sharing (FSS),
allowing it to achieve up to an order of magnitude better
performance compared to previous private query systems. We develop
protocols to execute complex queries
with low computation and bandwidth. As a proof of concept,
we have evaluated Splinter with three sample applications---a Yelp clone,
map routing, and flight search---and showed
that Splinter has low response times from 50 ms to 1.6 seconds with low
hosting costs.
